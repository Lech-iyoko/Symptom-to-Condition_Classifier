# Symptom-to-Condition_Classifier
---
An interactive web application that uses a machine learning pipeline to classify a potential medical condition based on user-described symptoms. This project demonstrates a complete MLOps workflow, from data analysis and model training to deployment and interactive UI creation.

**[Live Demo]** - *(Link to the deployed Streamlit Community Cloud app will go here when ready)*

![App Screenshot](![Screenshot (97)](https://github.com/user-attachments/assets/183952c1-d23d-4301-9c6e-c62140ec880e)

## Table of Contents
- [Project Overview](#project-overview)
- [The Journey: A Data-Centric Approach](#the-journey-a-data-centric-approach)
- [Final Model Performance](#final-model-performance)
- [Technology Stack](#technology-stack)
- [How to Run Locally](#how-to-run-locally)
- [Key Learnings](#key-learnings)

## Project Overview

This project's goal was to build a multi-class text classification model to assist in identifying potential medical conditions from symptom descriptions. The final deliverable is a web application, built with Streamlit, that allows for real-time inference by leveraging a trained LightGBM model whose features are generated by a `Bio_ClinicalBERT` transformer.

## The Journey: A Data-Centric Approach

The project began with a noisy, real-world dataset (`BI55/MedText`) that required significant data cleaning and heuristic-based labeling. This initial phase resulted in a performance ceiling with a Macro F1-score of ~0.43.

Recognizing that data quality was the primary bottleneck, a strategic pivot was made to the pre-cleaned and balanced `gretelai/symptom_to_diagnosis` dataset. This decision was the single most impactful change in the project, enabling the development of high-performing models and proving that **a data-centric approach is often more valuable than pure model optimization.**

## Final Model Performance

Two advanced modeling strategies were tested on the new dataset. The final results show that the feature-extraction pipeline with LightGBM outperformed the end-to-end fine-tuning approach for this specific problem.

| **Dataset** | **Embedding Strategy** | **Algorithm** | **Accuracy** | **Macro F1** |
| :--------------- | :------------------------------- | :----------------- | :----------- | :----------- |
| BI55/MedText     | Bio_ClinicalBERT (CLS)           | Linear SVC         | 0.46         | 0.43         |
| **GretelAI/S2D** | **Bio_ClinicalBERT (Mean Pooled)** | **LightGBM** | **0.835** | **0.834** |
| **GretelAI/S2D** | End-to-End Fine-Tuning           | Bio_ClinicalBERT   | 0.792        | 0.764        |


## Technology Stack

- **ML & Data Science:** PyTorch, Transformers, LightGBM, Scikit-learn, Pandas
- **Deployment & UI:** Streamlit, Hugging Face Hub, Joblib
- **Development Environment:** GitHub Codespaces

## How to Run Locally

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/](https://github.com/)<YourUsername>/symptom-classifier-app.git
    cd symptom-classifier-app
    ```

2.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

3.  **Run the Streamlit app:**
    ```bash
    streamlit run app.py
    ```

## Key Learnings

- **Data Quality is King:** The switch to a clean dataset provided a >40% performance increase, far exceeding any gains from model tuning.
- **Simplicity Can Outperform Complexity:** On this dataset, a well-regularized classical model (LightGBM) using high-quality embeddings generalized better than a more complex, end-to-end fine-tuned model.
- **The Importance of Granular Error Analysis:** Moving beyond aggregate metrics to analyze the confusion matrix was crucial in understanding *why* one model was better than another.

